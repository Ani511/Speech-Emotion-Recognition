{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6821e104-e184-427d-a00f-f1e56006fdc4",
   "metadata": {},
   "source": [
    "# ðŸ““ CNN-Based Speech Emotion Recognition using MFCC (RAVDESS Dataset)\n",
    "\n",
    "This notebook implements a **Convolutional Neural Network (CNN)** model to classify speech emotions using **MFCC features** extracted from the **RAVDESS** dataset. The workflow involves audio preprocessing, MFCC feature extraction, data preparation, CNN model training, and performance evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ Workflow Overview\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **1. Dataset** | RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) |\n",
    "| **2. Preprocessing** | Load `.wav` files, extract 40 MFCC features per file |\n",
    "| **3. Labels** | Emotions extracted from filename â†’ 8 classes: `neutral`, `calm`, `happy`, `sad`, `angry`, `fearful`, `disgust`, `surprised` |\n",
    "| **4. Feature Shape** | MFCC mean vectors â†’ `(40,)`, reshaped to `(40, 1)` for CNN |\n",
    "| **5. Model Architecture** | 2 Conv1D layers + BatchNorm + MaxPool + Dropout + Dense |\n",
    "| **6. Loss Function** | Categorical Crossentropy |\n",
    "| **7. Optimizer** | Adam (lr=0.001) with ReduceLROnPlateau + EarlyStopping |\n",
    "| **8. Evaluation** | Final accuracy on validation and test set |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Model Architecture\n",
    "\n",
    "```text\n",
    "Input: (40, 1)\n",
    "â”‚\n",
    "â”œâ”€â”€ Conv1D(64, kernel_size=5, activation='relu')\n",
    "â”œâ”€â”€ BatchNormalization\n",
    "â”œâ”€â”€ MaxPooling1D(pool_size=2)\n",
    "â”œâ”€â”€ Dropout(0.3)\n",
    "â”‚\n",
    "â”œâ”€â”€ Conv1D(128, kernel_size=5, activation='relu')\n",
    "â”œâ”€â”€ BatchNormalization\n",
    "â”œâ”€â”€ MaxPooling1D(pool_size=2)\n",
    "â”œâ”€â”€ Dropout(0.3)\n",
    "â”‚\n",
    "â”œâ”€â”€ Flatten\n",
    "â”œâ”€â”€ Dense(128, activation='relu')\n",
    "â”œâ”€â”€ Dropout(0.3)\n",
    "â””â”€â”€ Dense(8, activation='softmax')   â† 8 emotion classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d47a75-6800-4a65-8b36-f6fd788081c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "36/36 [==============================] - 4s 33ms/step - loss: 2.5778 - accuracy: 0.1788 - val_loss: 1.9897 - val_accuracy: 0.2257 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 1.9499 - accuracy: 0.2578 - val_loss: 1.9050 - val_accuracy: 0.3160 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 1.8619 - accuracy: 0.3047 - val_loss: 1.8462 - val_accuracy: 0.3229 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 1.8212 - accuracy: 0.3168 - val_loss: 1.8303 - val_accuracy: 0.3472 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 1.7310 - accuracy: 0.3507 - val_loss: 1.7309 - val_accuracy: 0.4097 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 1.6902 - accuracy: 0.3698 - val_loss: 1.6345 - val_accuracy: 0.4167 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 1.6272 - accuracy: 0.4158 - val_loss: 1.6152 - val_accuracy: 0.4618 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 1.5656 - accuracy: 0.4158 - val_loss: 1.5041 - val_accuracy: 0.4826 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 1.4938 - accuracy: 0.4488 - val_loss: 1.4226 - val_accuracy: 0.4861 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.4649 - accuracy: 0.4514 - val_loss: 1.4363 - val_accuracy: 0.4792 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 1.4013 - accuracy: 0.4688 - val_loss: 1.3486 - val_accuracy: 0.5035 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 1.3299 - accuracy: 0.5130 - val_loss: 1.2844 - val_accuracy: 0.5660 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.3413 - accuracy: 0.4922 - val_loss: 1.3351 - val_accuracy: 0.5035 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 1.2469 - accuracy: 0.5391 - val_loss: 1.2773 - val_accuracy: 0.5243 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2241 - accuracy: 0.5417 - val_loss: 1.2579 - val_accuracy: 0.5347 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "36/36 [==============================] - 1s 21ms/step - loss: 1.1808 - accuracy: 0.5781 - val_loss: 1.2404 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 1.1166 - accuracy: 0.5781 - val_loss: 1.2178 - val_accuracy: 0.5660 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 1.0443 - accuracy: 0.6319 - val_loss: 1.1765 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "36/36 [==============================] - 1s 40ms/step - loss: 0.9913 - accuracy: 0.6302 - val_loss: 1.2049 - val_accuracy: 0.5590 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 0.9897 - accuracy: 0.6510 - val_loss: 1.1512 - val_accuracy: 0.5868 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.9569 - accuracy: 0.6580 - val_loss: 1.1449 - val_accuracy: 0.6007 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 0.8986 - accuracy: 0.6788 - val_loss: 1.1475 - val_accuracy: 0.6007 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.8703 - accuracy: 0.6866 - val_loss: 1.1167 - val_accuracy: 0.6181 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8221 - accuracy: 0.7023 - val_loss: 1.1372 - val_accuracy: 0.5799 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.8313 - accuracy: 0.7109 - val_loss: 1.0825 - val_accuracy: 0.6111 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.7851 - accuracy: 0.7205 - val_loss: 1.0576 - val_accuracy: 0.6319 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "36/36 [==============================] - 2s 47ms/step - loss: 0.7515 - accuracy: 0.7127 - val_loss: 1.0892 - val_accuracy: 0.6285 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 0.6806 - accuracy: 0.7561 - val_loss: 1.0384 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6416 - accuracy: 0.7821 - val_loss: 1.0306 - val_accuracy: 0.6597 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.6664 - accuracy: 0.7491 - val_loss: 1.0479 - val_accuracy: 0.6528 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6041 - accuracy: 0.7743 - val_loss: 0.9823 - val_accuracy: 0.6701 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6174 - accuracy: 0.7691 - val_loss: 1.0126 - val_accuracy: 0.6701 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.6228 - accuracy: 0.7734 - val_loss: 0.9675 - val_accuracy: 0.6701 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.5350 - accuracy: 0.8203 - val_loss: 0.9845 - val_accuracy: 0.6389 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6200 - accuracy: 0.7734 - val_loss: 0.9626 - val_accuracy: 0.6389 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.5699 - accuracy: 0.8030 - val_loss: 0.9859 - val_accuracy: 0.6562 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.5323 - accuracy: 0.8168 - val_loss: 1.0176 - val_accuracy: 0.6493 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "35/36 [============================>.] - ETA: 0s - loss: 0.5111 - accuracy: 0.8330\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5134 - accuracy: 0.8307 - val_loss: 0.9948 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.4752 - accuracy: 0.8429 - val_loss: 0.9821 - val_accuracy: 0.6493 - lr: 5.0000e-04\n",
      "Epoch 40/50\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4381 - accuracy: 0.8472 - val_loss: 0.9610 - val_accuracy: 0.6493 - lr: 5.0000e-04\n",
      "Epoch 41/50\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4047 - accuracy: 0.8585 - val_loss: 0.9710 - val_accuracy: 0.6597 - lr: 5.0000e-04\n",
      "Epoch 42/50\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3682 - accuracy: 0.8663 - val_loss: 0.9645 - val_accuracy: 0.6701 - lr: 5.0000e-04\n",
      "Epoch 43/50\n",
      "35/36 [============================>.] - ETA: 0s - loss: 0.3560 - accuracy: 0.8750\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3546 - accuracy: 0.8750 - val_loss: 0.9819 - val_accuracy: 0.6562 - lr: 5.0000e-04\n",
      "Epoch 44/50\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3692 - accuracy: 0.8707 - val_loss: 0.9748 - val_accuracy: 0.6597 - lr: 2.5000e-04\n",
      "Epoch 45/50\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3680 - accuracy: 0.8628 - val_loss: 0.9721 - val_accuracy: 0.6493 - lr: 2.5000e-04\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.9610 - accuracy: 0.6493\n",
      "\n",
      "âœ… Final Test Accuracy: 64.93%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Point to your extracted RAVDESS dataset folder\n",
    "DATA_PATH = \"data\"  # e.g., \"./ravdess/\"\n",
    "\n",
    "emotion_map = {\n",
    "    1: \"neutral\", 2: \"calm\", 3: \"happy\", 4: \"sad\",\n",
    "    5: \"angry\", 6: \"fearful\", 7: \"disgust\", 8: \"surprised\"\n",
    "}\n",
    "\n",
    "def extract_features(file_path):\n",
    "    audio, sample_rate = librosa.load(file_path, duration=3, offset=0.5)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    return np.mean(mfccs.T, axis=0)\n",
    "\n",
    "# Prepare data\n",
    "file_paths, labels = [], []\n",
    "for root, _, files in os.walk(DATA_PATH):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            emotion_id = int(file.split(\"-\")[2])\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "            labels.append(emotion_map[emotion_id])\n",
    "\n",
    "X = np.array([extract_features(fp) for fp in file_paths])\n",
    "y = LabelEncoder().fit_transform(labels)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(64, 5, activation='relu', input_shape=(40, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, factor=0.5)\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[lr_scheduler, early_stopper]\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nâœ… Final Test Accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d2fe1a-174a-472a-9004-3933baba9bac",
   "metadata": {},
   "source": [
    "### Summary: CNN on MFCC Features (RAVDESS)\n",
    "\n",
    "| Stage              | Status                  |\n",
    "|--------------------|--------------------------|\n",
    "| **Feature Shape**   | `(samples, 40, 1)`       |\n",
    "| **Model Type**      | `1D CNN`                 |\n",
    "| **Architecture**    | Conv1D â†’ MaxPool â†’ Dropout (Ã—2) â†’ Flatten â†’ Dense(128) â†’ Softmax |\n",
    "| **Final Train Acc** | ~87.5%                   |\n",
    "| **Final Val Acc**   | ~67%                     |\n",
    "| **Final Test Acc**  | **64.93%**               |\n",
    "| **Dataset**         | RAVDESS (8 emotions)     |\n",
    "| **Feature Type**    | Mean MFCC (n_mfcc=40)    |\n",
    "| **Duration Offset** | 3 sec duration, 0.5 sec offset |\n",
    "| **Saved Model**     | _Not saved in this script_ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c53b800-0bcb-4dbc-8041-75e1e13ae4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CNN model saved as 'streamlit_app/emotion_cnn_model.h5'\n",
      "âœ… Label encoder saved as 'streamlit_app/cnn_label_encoder.pkl'\n"
     ]
    }
   ],
   "source": [
    "# âœ… Save model\n",
    "model.save(\"streamlit_app/emotion_cnn_model.h5\")\n",
    "print(\"âœ… CNN model saved as 'streamlit_app/emotion_cnn_model.h5'\")\n",
    "\n",
    "# âœ… Save label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Refit encoder on original labels to save it\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "# Save the label encoder\n",
    "joblib.dump(label_encoder, \"streamlit_app/cnn_label_encoder.pkl\")\n",
    "print(\"âœ… Label encoder saved as 'streamlit_app/cnn_label_encoder.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625d396b-5cfe-414b-a317-7faafc81f5cb",
   "metadata": {},
   "source": [
    "# ðŸ““ 2D CNN for Speech Emotion Recognition using Spectrograms\n",
    "\n",
    "This notebook builds a **2D Convolutional Neural Network** for classifying **6 emotions** from speech using **log-Mel Spectrograms** extracted from the **RAVDESS** dataset. The model processes audio signals into spectrogram images and learns to classify emotion categories.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ Workflow Overview\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **1. Dataset** | RAVDESS (subset: 6 emotions only) |\n",
    "| **2. Preprocessing** | Log-mel spectrograms padded/cropped to (128Ã—128) |\n",
    "| **3. Labels Used** | `neutral`, `calm`, `happy`, `sad`, `angry`, `fearful` |\n",
    "| **4. Feature Shape** | Each input sample shape: `(128, 128, 1)` |\n",
    "| **5. Model Architecture** | 2D CNN with 3 Conv-BN-Pool-Dropout blocks |\n",
    "| **6. Loss Function** | Categorical Crossentropy |\n",
    "| **7. Optimizer** | Adam (lr=0.001) |\n",
    "| **8. Callbacks** | EarlyStopping, ReduceLROnPlateau |\n",
    "| **9. Evaluation** | Model evaluated on 20% holdout test set |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Model Architecture\n",
    "\n",
    "```text\n",
    "Input: (128, 128, 1)\n",
    "â”‚\n",
    "â”œâ”€â”€ Conv2D(32, kernel_size=3x3, activation='relu')\n",
    "â”œâ”€â”€ BatchNormalization\n",
    "â”œâ”€â”€ MaxPooling2D(pool_size=2x2)\n",
    "â”œâ”€â”€ Dropout(0.3)\n",
    "\n",
    "â”œâ”€â”€ Conv2D(64, kernel_size=3x3, activation='relu')\n",
    "â”œâ”€â”€ BatchNormalization\n",
    "â”œâ”€â”€ MaxPooling2D(pool_size=2x2)\n",
    "â”œâ”€â”€ Dropout(0.3)\n",
    "\n",
    "â”œâ”€â”€ Conv2D(128, kernel_size=3x3, activation='relu')\n",
    "â”œâ”€â”€ BatchNormalization\n",
    "â”œâ”€â”€ MaxPooling2D(pool_size=2x2)\n",
    "â”œâ”€â”€ Dropout(0.4)\n",
    "\n",
    "â”œâ”€â”€ Flatten\n",
    "â”œâ”€â”€ Dense(128, activation='relu')\n",
    "â”œâ”€â”€ Dropout(0.4)\n",
    "â””â”€â”€ Dense(6, activation='softmax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5282783-daac-4650-a973-844bba2c8a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "27/27 [==============================] - 47s 2s/step - loss: 6.1655 - accuracy: 0.2903 - val_loss: 37.3802 - val_accuracy: 0.1792 - lr: 0.0010\n",
      "Epoch 2/40\n",
      "27/27 [==============================] - 35s 1s/step - loss: 1.5871 - accuracy: 0.3744 - val_loss: 2.2627 - val_accuracy: 0.2594 - lr: 0.0010\n",
      "Epoch 3/40\n",
      "27/27 [==============================] - 35s 1s/step - loss: 1.5489 - accuracy: 0.3768 - val_loss: 1.8480 - val_accuracy: 0.3066 - lr: 0.0010\n",
      "Epoch 4/40\n",
      "27/27 [==============================] - 32s 1s/step - loss: 1.4927 - accuracy: 0.3720 - val_loss: 1.7512 - val_accuracy: 0.3585 - lr: 0.0010\n",
      "Epoch 5/40\n",
      "27/27 [==============================] - 35s 1s/step - loss: 1.4410 - accuracy: 0.4171 - val_loss: 1.5907 - val_accuracy: 0.3443 - lr: 0.0010\n",
      "Epoch 6/40\n",
      "27/27 [==============================] - 34s 1s/step - loss: 1.4166 - accuracy: 0.4230 - val_loss: 1.6673 - val_accuracy: 0.3443 - lr: 0.0010\n",
      "Epoch 7/40\n",
      "27/27 [==============================] - 54s 2s/step - loss: 1.3998 - accuracy: 0.4289 - val_loss: 1.5527 - val_accuracy: 0.3491 - lr: 0.0010\n",
      "Epoch 8/40\n",
      "27/27 [==============================] - 36s 1s/step - loss: 1.3186 - accuracy: 0.4491 - val_loss: 1.5336 - val_accuracy: 0.3538 - lr: 0.0010\n",
      "Epoch 9/40\n",
      "27/27 [==============================] - 39s 1s/step - loss: 1.3792 - accuracy: 0.4135 - val_loss: 1.5891 - val_accuracy: 0.3915 - lr: 0.0010\n",
      "Epoch 10/40\n",
      "27/27 [==============================] - 36s 1s/step - loss: 1.3042 - accuracy: 0.4491 - val_loss: 2.7588 - val_accuracy: 0.2453 - lr: 0.0010\n",
      "Epoch 11/40\n",
      "27/27 [==============================] - ETA: 0s - loss: 1.2537 - accuracy: 0.4443\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "27/27 [==============================] - 35s 1s/step - loss: 1.2537 - accuracy: 0.4443 - val_loss: 1.7030 - val_accuracy: 0.3396 - lr: 0.0010\n",
      "Epoch 12/40\n",
      "27/27 [==============================] - 33s 1s/step - loss: 1.1915 - accuracy: 0.4597 - val_loss: 1.5702 - val_accuracy: 0.3491 - lr: 5.0000e-04\n",
      "Epoch 13/40\n",
      "27/27 [==============================] - 33s 1s/step - loss: 1.1513 - accuracy: 0.4822 - val_loss: 1.6814 - val_accuracy: 0.3962 - lr: 5.0000e-04\n",
      "Epoch 14/40\n",
      "27/27 [==============================] - 36s 1s/step - loss: 1.1555 - accuracy: 0.4976 - val_loss: 1.3250 - val_accuracy: 0.4481 - lr: 5.0000e-04\n",
      "Epoch 15/40\n",
      "27/27 [==============================] - 33s 1s/step - loss: 1.1586 - accuracy: 0.4834 - val_loss: 1.3070 - val_accuracy: 0.4387 - lr: 5.0000e-04\n",
      "Epoch 16/40\n",
      "27/27 [==============================] - 32s 1s/step - loss: 1.1016 - accuracy: 0.5273 - val_loss: 1.7717 - val_accuracy: 0.4481 - lr: 5.0000e-04\n",
      "Epoch 17/40\n",
      "27/27 [==============================] - 33s 1s/step - loss: 1.0693 - accuracy: 0.5332 - val_loss: 1.2946 - val_accuracy: 0.4717 - lr: 5.0000e-04\n",
      "Epoch 18/40\n",
      "27/27 [==============================] - 32s 1s/step - loss: 1.0827 - accuracy: 0.5474 - val_loss: 1.5553 - val_accuracy: 0.3915 - lr: 5.0000e-04\n",
      "Epoch 19/40\n",
      "27/27 [==============================] - 38s 1s/step - loss: 1.0131 - accuracy: 0.5664 - val_loss: 1.5436 - val_accuracy: 0.4340 - lr: 5.0000e-04\n",
      "Epoch 20/40\n",
      "27/27 [==============================] - ETA: 0s - loss: 1.0199 - accuracy: 0.5664\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "27/27 [==============================] - 33s 1s/step - loss: 1.0199 - accuracy: 0.5664 - val_loss: 1.6098 - val_accuracy: 0.4434 - lr: 5.0000e-04\n",
      "Epoch 21/40\n",
      "27/27 [==============================] - 36s 1s/step - loss: 0.9841 - accuracy: 0.5865 - val_loss: 1.6102 - val_accuracy: 0.4717 - lr: 2.5000e-04\n",
      "Epoch 22/40\n",
      "27/27 [==============================] - 30s 1s/step - loss: 0.9304 - accuracy: 0.5889 - val_loss: 1.6647 - val_accuracy: 0.4340 - lr: 2.5000e-04\n",
      "Epoch 23/40\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.9198 - accuracy: 0.6019\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "27/27 [==============================] - 30s 1s/step - loss: 0.9198 - accuracy: 0.6019 - val_loss: 1.5027 - val_accuracy: 0.5047 - lr: 2.5000e-04\n",
      "7/7 [==============================] - 3s 332ms/step - loss: 1.2946 - accuracy: 0.4717\n",
      "\n",
      "âœ… Final Accuracy with 2D CNN & Spectrograms: 47.17%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Emotion mapping - Use only 6 for now\n",
    "EMOTION_MAP = {\n",
    "    1: \"neutral\", 2: \"calm\", 3: \"happy\", 4: \"sad\",\n",
    "    5: \"angry\", 6: \"fearful\"\n",
    "}\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "\n",
    "# Spectrogram extractor\n",
    "def extract_spectrogram(file_path, max_pad_len=128):\n",
    "    y, sr = librosa.load(file_path, duration=3, offset=0.5)\n",
    "    melspec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    logspec = librosa.power_to_db(melspec)\n",
    "\n",
    "    if logspec.shape[1] < max_pad_len:\n",
    "        pad_width = max_pad_len - logspec.shape[1]\n",
    "        logspec = np.pad(logspec, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        logspec = logspec[:, :max_pad_len]\n",
    "\n",
    "    return logspec\n",
    "\n",
    "# Data load\n",
    "X, y = [], []\n",
    "for root, _, files in os.walk(DATA_PATH):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            try:\n",
    "                emotion_id = int(file.split(\"-\")[2])\n",
    "                if emotion_id in EMOTION_MAP:\n",
    "                    label = EMOTION_MAP[emotion_id]\n",
    "                    spect = extract_spectrogram(os.path.join(root, file))\n",
    "                    X.append(spect)\n",
    "                    y.append(label)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Reshape for CNN [samples, height, width, channels]\n",
    "X = X[..., np.newaxis]\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 2D CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=X.shape[1:]),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(len(le.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, factor=0.5),\n",
    "    EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=40,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nâœ… Final Accuracy with 2D CNN & Spectrograms: {acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ed7c59-05c0-4a68-a900-9d6a88ad5466",
   "metadata": {},
   "source": [
    "## âœ… Training Summary\n",
    "\n",
    "| Metric               | Value                           |\n",
    "|----------------------|----------------------------------|\n",
    "| **Model Type**        | 2D CNN with Log-Mel Spectrograms |\n",
    "| **Feature Shape**     | (128, 128, 1)                    |\n",
    "| **Emotion Classes**   | 6 (`neutral`, `calm`, `happy`, `sad`, `angry`, `fearful`) |\n",
    "| **Train Accuracy**    | ~60%+ (peak)                    |\n",
    "| **Val Accuracy**      | Peaked at ~50.47%               |\n",
    "| **Test Accuracy**     | **47.17%**                      |\n",
    "| **Loss Function**     | Categorical Crossentropy        |\n",
    "| **Optimizer**         | Adam (lr=0.001 â†’ adaptive)      |\n",
    "| **Callbacks Used**    | EarlyStopping, ReduceLROnPlateau |\n",
    "| **Training Duration** | 23 epochs (early stop)          |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e385d1b-e2ab-4673-8b17-7c003d7b7502",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save Model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstreamlit_app/emotion_cnn_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Model saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion_cnn_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save Model\n",
    "# --------------------------\n",
    "model.save(\"streamlit_app/emotion_cnn_model.h5\")\n",
    "print(\"âœ… Model saved as 'emotion_cnn_model.h5'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988805db-11e2-4330-8ada-257a1ad88595",
   "metadata": {},
   "source": [
    "## Project Summary: Speech Emotion Recognition\n",
    "\n",
    "This notebook implements two deep learning pipelines to classify emotional states from speech using the RAVDESS dataset:\n",
    "\n",
    "1. **1D Conv + BiLSTM on MFCCs**\n",
    "2. **2D CNN on Log-Mel Spectrograms**\n",
    "\n",
    "---\n",
    "\n",
    "### Results Summary\n",
    "\n",
    "| Model                    | Input Features     | Train Accuracy | Val Accuracy | Final Test Accuracy |\n",
    "|--------------------------|--------------------|----------------|--------------|---------------------|\n",
    "| **1D Conv + BiLSTM**     | MFCC (40,)         | ~87%           | ~70%         | **64.93%**          |\n",
    "| **2D CNN**               | Log-Mel Spectrogram | ~60%+          | ~50.47%      | **47.17%**          |\n",
    "\n",
    "---\n",
    "\n",
    "### Model Details\n",
    "\n",
    "#### 1D Conv + BiLSTM\n",
    "- **Features**: MFCCs (mean pooled)\n",
    "- **Architecture**: Conv1D â†’ BiLSTM â†’ Dense\n",
    "- **Label Encoder**: `lstm_label_encoder.pkl`\n",
    "- **Saved Model**: `emotion_lstm_model.h5`\n",
    "- **Observations**:\n",
    "  - Captures temporal dynamics using LSTM.\n",
    "  - Stronger performance despite simpler input.\n",
    "\n",
    "#### 2D CNN (Spectrogram)\n",
    "- **Features**: Log-Mel Spectrograms (128Ã—128)\n",
    "- **Architecture**: Stacked Conv2D + BatchNorm + MaxPool + Dense\n",
    "- **Observations**:\n",
    "  - Spectrograms offer richer frequency-time resolution.\n",
    "  - Lower accuracy due to model complexity and possible overfitting.\n",
    "  - Potential improvements with pretrained backbones or hybrid models.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- The **1D Conv + BiLSTM** model **outperformed** the 2D CNN despite using lower-dimensional MFCC features. Temporal modeling with LSTM helped capture speech patterns better.\n",
    "- The **2D CNN on spectrograms** showed potential but underperformed, likely due to limited data, high input dimensionality, and lack of transfer learning.\n",
    "- Both models highlight the impact of **input representation** and **architecture choice** in speech emotion recognition.\n",
    "- Future improvements may include:\n",
    "  - **Data augmentation** (pitch shift, noise)\n",
    "  - **Transfer learning** with audio-pretrained CNNs (e.g., VGGish, YAMNet)\n",
    "  - **Hybrid models** (CNN + LSTM)\n",
    "  - **Attention mechanisms** for temporal focus\n",
    "\n",
    "---\n",
    "\n",
    "This notebook serves as a strong baseline for building and comparing deep learning architectures on raw audio emotion datasets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
